# Speech Emotion Recognition ğŸ¤ğŸ˜„ğŸ˜¡ğŸ˜¢

[![Python](https://img.shields.io/badge/Python-3.7%2B-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![Star](https://img.shields.io/github/stars/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition?style=social)](https://github.com/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition)
[![Issues](https://img.shields.io/github/issues/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition)](https://github.com/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition/issues)

Recognize human emotions from speech audio using cutting-edge Deep Learning and Signal Processing. This project automatically classifies emotions such as **Happy**, **Angry**, **Sad**, **Neutral**, and more from `.wav` files or microphone input.

---

## ğŸš€ Features

- ğŸ¶ **Audio Input**: Supports `.wav` files and live microphone.
- ğŸ§  **Deep Learning Models**: Use CNN, LSTM, or hybrid architectures for best results.
- ğŸ“Š **Interactive Visualizations**: Confusion matrix, prediction plots, live stream graphs.
- ğŸ—£ **Real-time Prediction** (Optional): See detected emotions instantly as you speak!
- ğŸ“¦ **Modular Code**: Easy to extend for extra emotions, multilanguage, or other models.

---

## ğŸ“¦ Installation

1. **Clone the repo:**
   ```bash
   git clone https://github.com/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition.git
   cd CodeAlpha_Project_Speech-Emotion-Recognition
   ```

2. **Create & activate a virtual environment (recommended):**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ§‘â€ğŸ’» Usage

### 1. **Predict Emotion from WAV File**

```bash
python predict.py --input path/to/audio.wav
```

### 2. **Real-Time Microphone Emotion Recognition**

```bash
python mic_demo.py
```

### 3. **Train (or Retrain) Models**

```bash
python train.py --data_dir path/to/dataset
```

---

## ğŸ¥ Demo

> **Add Demo GIFs or Screenshots here!**
>
> ![Demo GIF](assets/demo.gif)

---

## ğŸ¤“ Dataset

- Uses open datasets like [RAVDESS](https://zenodo.org/record/1188976#.YjzolmjTXIV), [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D), or your own `.wav` files.
- **Tip:** Place your dataset inside the `data/` folder and update config paths as needed.

---

## âš™ï¸ Configuration

You can modify parameters like model type, number of epochs, batch size, etc., in `config.py`.

---

## ğŸ“ Contributing

Pull requests, suggestions, and questions are welcome!  
See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

---

## ğŸ’¬ Issues & Discussions

- [Open an Issue](https://github.com/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition/issues)
- [Start a Discussion](https://github.com/abhijit869/CodeAlpha_Project_Speech-Emotion-Recognition/discussions)

---

## ğŸ“œ License

This repo is licensed under the [MIT License](./LICENSE).

---

## ğŸ™Œ Acknowledgments

- [TensorFlow](https://www.tensorflow.org/)
- [Librosa](https://librosa.org/)
- [Scikit-learn](https://scikit-learn.org/)
- Audio datasets: RAVDESS, CREMA-D

---

### â­ï¸ Star this repo if you find it useful!



